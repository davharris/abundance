
import pandas as pd
import scipy
import numpy as np
from scipy import stats, special

import tensorflow as tf
print(tf.__version__)
sess = tf.InteractiveSession()

import tfnb

import matplotlib
%matplotlib inline
import matplotlib.pyplot as plt

N_z = 20  # Latent variables, split evenly between sites & observers
N_h = 50 # Neurons in non-final hidden layer(s)
N_bottleneck = 10 # Neurons in final hidden layer
N_reps = 1

# Data
x_array = np.array(pd.read_csv("x.csv"))
y_array = np.array(pd.read_csv("y.csv"))
site_array = (np.array(pd.read_csv("site.csv"))) - 1 # Zero-indexing versus R's one-indexing
obs_array = (np.array(pd.read_csv("obs.csv"))) - 1   # Zero-indexing

# Sizes
N_x = x_array.shape[1]
N_y = y_array.shape[1]

N_s = int(np.max(site_array[:,1])) + 1 # Include zero as a column
N_o = int(np.max(obs_array[:,1])) + 1  # Include zero as a column
N_rows = x_array.shape[0]

N = tf.placeholder(tf.int32, shape=[], name = "rows") # Minibatch size
Y = tf.placeholder(tf.float32, shape=[None, N_y], name = "abundance") # Response variables
X = tf.placeholder(tf.float32, shape=[None, N_x], name = "environment") # Predictor variables
S = tf.placeholder(tf.int32, shape=[None], name = "Site-ID") # site index
O = tf.placeholder(tf.int32, shape=[None], name = "Observer-ID") # observer index

def mu_initializer(*args, **kwargs):
    out = 1.5 * np.log(np.mean(y_array,axis=0)) + 1
    return out
def size_initializer(*args, **kwargs):
    out = scipy.special.logit(np.mean(y_array!=0, axis=0)) / 2.0
    return out
def zi_p_initializer(*args, **kwargs):
    out = scipy.special.logit((np.mean(y_array == 0, axis=0))) / 2.0 - 1.0
    return out

with tf.variable_scope("latent-params"):
    half_N_z = int(N_z/2)
    site_mu = tf.Variable(tf.random_normal([N_s, N_z]) * tf.sqrt(0.1))
    obs_mu = tf.Variable(tf.random_normal([N_o, N_z]) * tf.sqrt(0.1))
    site_sigma = tf.Variable(tf.ones([N_s, N_z]))
    obs_sigma = tf.Variable(tf.ones([N_o, N_z]))
    mu0 = tf.add(tf.gather(site_mu, S), tf.gather(obs_mu, O), name="mu")
    sigma0 = tf.sqrt(tf.add(tf.square(tf.gather(site_sigma,S)), tf.square(tf.gather(obs_sigma, O))), name = "sigma")

with tf.variable_scope("latent-random"):
    epsilon = tf.random_normal([N, N_z])
    Z = tf.add(mu0, sigma0 * epsilon, name = "Z")
    
# Define layer 1's inputs with X and Z
XZ = tf.concat([X, Z], 1)

H1 = tf.layers.dense(XZ, 
                     units=N_h, 
                     activation=tf.nn.elu,
                     kernel_initializer=tf.contrib.layers.xavier_initializer(),
                     kernel_regularizer = tf.contrib.layers.l2_regularizer(0.01),
                     name="layer1")
H2 = tf.layers.dense(H1, 
                     units=N_bottleneck, 
                     activation=tf.nn.elu,
                     kernel_initializer=tf.contrib.layers.xavier_initializer(),
                     kernel_regularizer = tf.contrib.layers.l2_regularizer(0.01),
                     name="layer2")
HN = H2

mu_optima = tf.Variable(tf.random_normal((N_bottleneck, N_y), stddev=1.0))
mu_scales = tf.Variable(8 * tf.ones((N_bottleneck, N_y)))

d_squared = tf.transpose(
                tf.map_fn(lambda i: 
                  tf.reduce_sum(tf.squared_difference(HN, mu_optima[:,i]) / tf.square(mu_scales[:,i]), 
                                axis=1), 
                  np.arange(N_y, dtype=np.int32),
                  dtype=tf.float32))

peak_raw = tf.Variable(np.mean(y_array, axis=0), dtype=tf.float32)
peak = tf.nn.softplus(peak_raw)

nb_mu = peak * tf.exp(-0.5 * d_squared) + 1E-6
nb_size_raw = tf.Variable(tf.ones(N_y))
nb_size = tf.exp(nb_size_raw) + 1

zi_p_raw = tf.Variable(-3 * tf.ones_like(nb_size))
zi_p = tf.sigmoid(zi_p_raw)


# Switch to N,p parameterization for negative binomial
with tf.variable_scope("negbin-prob"):
    nb_p = nb_size / (nb_mu + nb_size)



with tf.variable_scope("lossses",reuse=False):
    prediction_loss = -tf.reduce_sum(tfnb.zi_nbinom_ll(Y, nb_size, nb_p, zi_p))

    # Put a prior on negative binomial's "p" and the zero-inflation parameter, 
    # which both cause numerical problems at 0 or 1
    prior_loss = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))

    variational_loss = tfnb.kl(tf.gather(site_mu, S), tf.gather(site_sigma, S)) + \
                                tfnb.kl(tf.gather(obs_mu, O), tf.gather(obs_sigma, O))

    loss = prediction_loss + prior_loss + variational_loss

adam = tf.train.AdamOptimizer();
train_step = adam.minimize(loss);

# Session incantations 
init = tf.global_variables_initializer()
sess.run(init)

# Save output for tensorboard
train_writer = tf.summary.FileWriter('./train', sess.graph)
train_writer.close()

rows = np.arange(y_array.shape[0])
n_steps = 0.
t = tf.placeholder(dtype=tf.float32)

class minibatcher(object):
    def __init__(self, nrow):
        self.nrow = nrow
        self.epochs = 0
        self.pointer = 0
        self.order = np.arange(nrow)
        np.random.shuffle(self.order)
        #self.print_status()
    def get_rows(self, n):
        if (n+self.pointer < self.nrow):
            out = self.order[self.pointer + np.arange(n)]
            self.pointer += n
            return out
        else:
            # This is suboptimal because it throws out anything at the end
            # of the epoch that doesn't divide evenly into `n`
            np.random.shuffle(self.order)
            self.pointer = 0
            self.epochs += 1
            self.print_loss()
            return self.get_rows(n)
    def print_loss(self):
        raw_losses = sess.run([prediction_loss, prior_loss, variational_loss, loss], 
                              feed_dict=full_feed())
        loss_list = [loss / N_rows for loss in raw_losses]
        print(np.round(loss_list, 4))
        return loss_list


def make_minibatch(n):
    if n==N_rows:
        rows = np.arange(N_rows)
    else:
        rows = np.repeat(mb.get_rows(n), N_reps)
    
    return {X:x_array[rows,:], 
           Y:y_array[rows,:], 
           S:site_array[rows,1],
           O:obs_array[rows,1],
           N:len(rows)}
def full_feed():
    return make_minibatch(N_rows)

mb = minibatcher(N_rows)

# Fit the model
n = 32

for i in range(3000):
    sess.run(train_step, feed_dict=make_minibatch(n))
    n_steps += 1

print(n_steps)
print(mb.epochs)
mb.print_loss();

plt.hist(sess.run(mu_optima, feed_dict=make_minibatch(1000)).flatten(), bins="fd");

plt.hist(sess.run(mu_scales, feed_dict=make_minibatch(1000)).flatten(), bins="fd");

plt.hist(sess.run(nb_size, feed_dict=make_minibatch(1000)).flatten(), bins="fd");

plt.hist(sess.run(zi_p, feed_dict=make_minibatch(1000)).flatten(), bins="fd");

plt.hist(sess.run(tf.exp(-0.5 * d_squared), feed_dict=make_minibatch(1000)).flatten(), bins="fd");

plt.hist(sess.run(obs_sigma).flatten(), bins = "fd");
plt.hist(sess.run(site_sigma).flatten(), bins = "fd");

plt.hist(sess.run(obs_mu).flatten(), bins = "fd");
plt.hist(sess.run(site_mu).flatten(), bins = "fd");

plt.scatter(np.mean(sess.run(nb_mu, feed_dict=make_minibatch(1000)), axis=0),
           np.mean(y_array, axis=0));

mus, losses = sess.run((nb_mu, tfnb.zi_nbinom_ll(Y, nb_size, nb_p, zi_p)), feed_dict=full_feed())
plt.hist(losses.flatten());

print(np.mean(losses))
print(np.median(losses))


